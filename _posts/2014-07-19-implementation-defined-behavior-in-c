---
layout: post
title: Implementation-defined Behavior in C
tags: C semantics
published: False
---

I've become interested recently in how we should think (and reason) about C programs that perform implementation-defined operations (in particular, conversions between nonnull pointers and integers). 

Mostly, this is just idle curiosity on my part. 

On the other hand, there are some interesting issues surrounding implementation-defined behavior, portability, and formal models of compiler correctness. I don't go deep into these issues here---this post is mostly just a high-level overview, with some unanswered questions.

First, definitions: 

### Implementation-defined Behavior

The C11 standard defines "implementation-defined behavior" as 
> unspecified behavior where each implementation documents how the choice is made.
> (3.4.1, [http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf](C11 draft standard))

### Unspecified Behavior 

"Unspecified behavior" (3.4.4), in turn, either has two or more possible specified outcomes (e.g., order of evaluation of function arguments), or involves the use of an "unspecified value," which is any valid value of the relevant type (int, float, etc.). 

### Examples

Implementation-defined:
* pointer-to-integer (and integer-to-pointer) conversions, whenever the pointer/integer is nonnull/nonzero
* representation of signed integers (two's complement, one's complement, etc.)
* the extent to which the compiler respects the inline function specifier
* ... and many more ... (J.3)

Unspecified:
* the order in which subexpressions are evaluated (and the order in which any resulting side-effects occur) in function calls, evaluation of &&, ||, ?:, and comma operators
* the order in which # and ## operations are evaluated during preprocessing
* ... and many more ... (J.1)

### Integer-Pointer Conversion

The most interesting kind of implementation-defined behavior, from my perspective, is conversion between (void, nonnull) pointers and integers, via explicit casts:

``` C
#include <stddef.h>

int g(int* x) { return ((uintptr_t)x <= 0xbffff980); }

static int f(void) {
  int a = 0;
  return g(&a);
}

int main(void) {  return f(); }
```

What integer does this program return? 

If we trace through the program, we pretty quickly get to the key bit:

``` C
return ((uintptr_t)x <= 0xbffff980)
```

Compare the address x (when viewed as an unsigned integer) less than or equal to a particular integer constant.
uintptr_t is an unsigned integer type guaranteed not to screw up the integer-pointer conversion.

Let's assume for a moment that we're trying to determine the result without reference to any particular compiler. This is actually a very reasonable thing to do---many C verification and bugfinding tools don't assume any particular compiler, for example.

The C standard says that the result of the conversion (uintptr_t)x is implementation defined. I.e., "The operation has unspecified behavior, where each implementation documents how the choice is made." 

We're not assuming any particular implementation, so the result of the conversion must just be unspecified, right? (If we want to treat the code as truly portable, we need to consider all possible implementations of pointer-to-integer casts.)

Which means that (uintptr_t)x is a well-defined but arbitrary valid unsigned integer (that is, any unsigned integer representable as a uintptr_t). All we can say, then, about the less-than-or-equal-to comparison is that it is either 0 or 1 (but we don't know which). The program is nondeterministic.

### The Problem with Implementation-defined Behavior

Things get a bit weirder if we start thinking about the behavior of the program above with reference to a particular C implementation, e.g., gcc, clang, or CompCert.

gcc, for example, documents pointer-to-integer casts as follows:

> asdasda
> asdasd
> asdasdasdasd

If we assume gcc, we should assume the behavior above.

Assuming gcc should at least resolve the nondeterminism we saw above, right? (Because we're fixing a particular implementation.)

We can experiment by compiling the program, running it, and checking its exit code. 

When compiled without optimizations, we get exit code 1.

> gcc -O0 f.c
> ./a.out
> echo $?
>   1

When compiled with optimizations turned on, we get exit code 0.

> gcc -O0 f.c
> ./a.out
> echo $?
>   0

What's going on?
In the second compilation, gcc inlines function f at f's callsite in main. Inlining, in turn, changes the absolute position on the stack at which local variable a is allocated, which causes the less-than-or-equal comparison to switch from 1 to 0. 
(To see this behavior on your own platform, you'll probably have to choose the integer 0xbffff980 a bit craftily, and turn address-space randomization off.)

The problem is: gcc's inlining phase (and many other optimizations) assume that we're not writing code like g above to observe the concrete addresses of C objects like a. But it's not obvious to me that such code is ruled out by the C standard. 

### CompCert

CompCert C follows a third path: get stuck on the following code:

#include <stdio.h>

int g(unsigned int* x) {
  return ((unsigned int)x <= 0xbffff980);
}

static inline unsigned int f(void) {
  unsigned int a = 0;
  return g(&a);
}

int main(void) {
  printf("%x\n", f());
  return 0;
}

If you run this in the CompCert C interpreter (ccomp -interp), you get:

Stuck state: in function g, expression <ptr> <= -1073743488
Stuck subexpression: <ptr> <= -1073743488
ERROR: Undefined behavior

at this point in g:

  return ((unsigned int)x <= 0xbffff980);

The cast to unsigned int leaves the Vptr a Vptr (because it's a neutral cast). The comparison then gets stuck.

A question: Is getting stuck a valid unspecified behavior?

(Incidentally, this program on my machine returns 0 or 1, depending on whether I tell CompCert to inline the function f :-))

So, my thinking in general is that nonportable C code is just bad :-)

